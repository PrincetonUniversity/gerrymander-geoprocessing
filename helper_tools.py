"""
Helper methods related to image processing and geoprocessing to be used in 
PDF conversion. 
"""
import pandas as pd
import pysal as ps
import geopandas as gpd
from PIL import Image
import os
import numpy as np
import math
import shapely as shp
from shapely.geometry import Polygon
from shapely.geometry import Point
from collections import Counter
import csv
import operator
import datetime
from titlecase import titlecase
import shutil

def generate_bounding_frame(df, file_str):
    ''' Generates and saves a bounding frame for the geometries in a dataframe
    to assist with autocropping.
    
    Arguments:
        df: geodataframe with geometries
        file_str: file path for bounding frame shapefile
        
    Output:
        Geometry of bounding frame (also saves shapefile)
    '''
    # Calculate boundaries of the geodataframe using union of geometries
    # takes form (min_x, min_y, max_x, max_y)
    bounds = shp.ops.cascaded_union(list(df['geometry'])).bounds
    xmin = bounds[0]
    xmax = bounds[2]
    xlen = xmax-xmin
    ymin = bounds[1]
    ymax = bounds[3]
    ylen = ymax-ymin
    
    # generate picture frame shape
    in_frame = Polygon([(xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax)])
    out_frame = Polygon([(xmin-10*xlen, ymin-10*ylen),\
                         (xmax+10*xlen, ymin-10*ylen),\
                         (xmax+10*xlen, ymax+10*ylen),\
                         (xmin-10*xlen, ymax+10*ylen)])
    frame = out_frame.symmetric_difference(in_frame)
    
    # create dataframe and save to shapefile
    out_df = gpd.GeoDataFrame()
    out_df['geometry'] = [frame]
    out_df.to_file(file_str)
    
    return frame

def cropped_bordered_image(img_arr):
    ''' Given an image with a border of one color, returns an image with the
    border cropped off.  Works with images in numpy array form.
    
    Argument:
        img_arr: numpy array generated by np.asarray(image)
        
    Output:
        modified image array with border cropped off
    '''
    
    # calcluate extents of image
    xlen = len(img_arr[0])
    ylen = len(img_arr)
    
    # calculate border color as top-left pixel
    color = img_arr[0][0]
    
    # if other corners are not the same color, throw an error
    if not ((img_arr[0][xlen-1]== color).all() and \
            (img_arr[ylen-1][xlen-1] == color).all() and \
            (img_arr[ylen-1][0] == color).all()):
        raise ValueError('Image does not have same color in all corners')
        
    
    # find inner border extents (left, top, right, bottom)
    top = 0
    while (img_arr[top] == color).all():
        top += 1
    bottom = ylen-1
    while (img_arr[bottom] == color).all():
        bottom -= 1
    left = 0
    while (img_arr[:,left] == color).all():
        left += 1
    right = xlen-1
    while (img_arr[:,right] == color).all():
        right -= 1
        
    # crop and return array
    return img_arr[top:bottom+1, left:right+1]

def real_rook_contiguity(df, geo_id = 'geometry',
                         nbr_id='neighbors',struct_type='list'):
    ''' Generates neighbor list using rook contiguity for a geodataframe.
    
    Arguments:
        df: geodataframe to apply rook contiguity to
        geo_id = column name for geometries in dataframe
        nbr_id = column name for neighbor list (to be generated) in dataframe
        struct_type: determines whether neighbors are returned as a list or
        as a dict
        
    Output:
        dataframe with neighbors list for each attribute in a new column
        called nbr_id (default name is 'neighbors')
    '''
    
    # Obtain queen continuity for each shape in the dataframe. We will remove 
    # all point contiguity. Shapely rook contiguity sometimes assumes lines
    # with small lines are points
    w = ps.weights.Queen.from_dataframe(df, geom_col=geo_id)
    
    # Initialize neighbors column
    df[nbr_id] = pd.Series(dtype=object)   
    
    # Initialize neighbors for each precinct
    for i,_ in df.iterrows():
        struct = w.neighbors[i]
        
        # Iterate through every precinct to remove all neighbors that only 
        # share a single point. Rook contiguity would asssume some lines are 
        # points, so we have to use queen and then remove points
        
        # Obtain degree (# neighbors) of precinct
        nb_len = len(struct)
        
        # Iterate through neighbor indexes in reverse order to prevent errors 
        # due to the deletion of elements
        for j in range(nb_len - 1, -1, -1):
            # get the jth neighbor
            j_nb = struct[j]
            
            # get the geometry for both precincts
            i_geom = df.at[i, geo_id]
            j_nb_geom = df.at[j_nb, geo_id]
            
            # If their intersection is a point, delete j_nb from i's neighbor 
            # list do not delete in both directions. That will be taken care of
            # eventually when i = j_nb later in the loop or before this occurs
            geom_type = i_geom.intersection(j_nb_geom).type
            if geom_type == 'Point' or geom_type == 'MultiPoint':
                del struct[j]
        
        # Assign to dataframe according to the structure passed in
        if struct_type == 'list':
            df.at[i, nbr_id] = struct
        elif struct_type == 'dict':
            df.at[i, nbr_id] = dict.fromkeys(struct)
    return df

def get_shared_perims(df):
    ''' Return a dataframe with a new field, neighbors, containing a dictionary
    where the keys are indices of rook-contiguous neighbors and the values are
    shared perimeters.
    
    Arguments:
        df:'''
    df = real_rook_contiguity(df, struct_type='dict')
    
    # iterate over all precincts to set shared_perims
    for i,_ in df.iterrows():
        
        # iterate over the neighbors of precinct i
        for key in df.at[i, 'neighbors']:
        
            # obtain the boundary between current precinct and its j neighbor
            shape = df.at[i, 'geometry'].intersection(df.at[key, 'geometry'])
            
            # get shared_perim length (casework)
            if shape.type == 'GeometryCollection' or \
                    shape.type == 'MultiLineString':
                length = 0
                for line in shape.geoms:
                    if line.type == 'LineString':
                        length += line.length
            elif shape.type == 'LineString':
                length = shape.length
            else:
                print(shape.type)
                print(i)
                print(key)
                print ('Unexpected boundary')
                length = -1
                
            df.at[i, 'neighbors'][key] = length
    return df

def reduce_colors(img, num_colors):
    ''' Generates an image reducing the number of colors to a number 
    specified by the user. Uses Image.convert from PIL.
    
    Arguments:
        img: original image in PIL Image format
        num_colors: number of distinct colors in output file
        
    Output:
        Modified image with reduced number of distinct RGB values'''
    
    conv_img = img.convert('P', palette=Image.ADAPTIVE, colors = num_colors)
    return conv_img.convert('RGB')

def merge_fully_contained(df, geo_id = 'geometry',
                          nbr_id='neighbors', cols_to_add=['area']):
    '''If any geometry is contained entirely within another geometry, this
    function merges it into the larger geometry.  Slightly distinct from the
    'donut and donut-hole' analogy because if multiple precincts are completely
    surrounded by a ring-shaped precinct, then they will all be consumed.
    
    Arguments:
        df: geodataframe to apply rook contiguity to
        geo_id = column name for geometries in dataframe
        nbr_id = column name for neighbor list (to be generated) in dataframe
        cols_to_add = which attributes (column names) should be added when 
            precincts are merged (i.e. area or population). For all other 
            columns, the data from the consuming precinct is preserved.
        
    Output:
        dataframe with neighbors list for each attribute in a new column
        called nbr_id (default name is 'neighbors')
    '''
    
    # create neighbor list if it does not exist
    if (nbr_id not in df.columns):
        df = real_rook_contiguity(df)
    
    # Create list of rows to drop at the end of the multiple contained check
    ids_to_drop = []
    
    # Iterate over all attributes
    for i,_ in df.iterrows():
        
        # Create polygon Poly from its exterior coordinates. This
        # polygon will be filled without an interior. The purpose of filling
        # the interior is to allow for an intersection to see if a neighbor is
        # fully contained
        geometry = df.at[i, geo_id]
        poly_coords = list(geometry.exterior.coords)
        poly = Polygon(poly_coords)
        
        # Assuming no overlaps in the geometries, if poly contains the 
        # geometry then no neighbors can be contained in the geometry.
        # So we can go quickly through the loop in most cases.
        if geometry.contains(poly):
            continue
        
        # Create list of contained neighbor id's to delete
        nb_ix_del = []
    
        # Define a list of "possibly contained" precincts. If a precinct
        # is nested witin other contained precincts, we will need to add it to
        # this list
        possibly_contained = df.at[i, nbr_id]
        for j in possibly_contained:        
            
            # Check if the intersection of Poly (precint i's full polygon) and the
            # current neighbor is equal to the current neighbor. This demonstrates
            # that the current neighbor is fully contained within precinct i
            j_geom = df.at[j, geo_id]
            
            if j_geom == j_geom.intersection(poly):
                # j is fully contained within i. To account for nested precincts
                # we append any neighbor of j that is not already in possibly_
                # contained not including i
                for j_nb in df.at[j, nbr_id]:
                    if j_nb not in possibly_contained and j_nb != i:
                        possibly_contained.append(j_nb)
                        
                # Add geometry of j to geometry of i
                polys = [df.at[i, 'geometry'], 
                         df.at[j, 'geometry']]
                df.at[i, 'geometry'] = shp.ops.cascaded_union(polys)
    
                # Add capture columns from neighbor to precinct i
                for col in cols_to_add:
                    if col in df.columns:
                        df.at[i, col] = df.at[i, col] + df.at[j, col]
                            
                # add neighbor reference from precinct i to delete if a neighbor
                if j in df.at[i, nbr_id]:
                    nb_ix = df.at[i, nbr_id].index(j)
                    nb_ix_del.append(nb_ix)
                
                # add neighbor precinct to the ID's to be dropped
                ids_to_drop.append(j)
                
        # Delete neighbor references from precinct i
        if len(nb_ix_del) > 0:
            # iterate through indexes in reverse to prevent errors through deletion
            for nb_ix in reversed(nb_ix_del):
                del(df.at[i, nbr_id][nb_ix])
    
    # Drop contained precincts from the dataframe and return
    df = df.drop(ids_to_drop)
    return df

    
def pt_to_pixel_color(pt, img_arr, xmin, xlen, ymin, ylen, img_xmin, img_xlen, 
                img_ymin, img_ylen):
    '''Returns the pixel color corresponding to a given Shapely point, given 
    that the geometry and image have been aligned.  Uses the bounds of the
    geometry to map the point to the proper indices in the image array.  Thus,
    the image array must come from an image that has been cropped to fit on all
    four sides.
    
    Arguments:
        pt: Shapely point within reference geometry
        img_arr: numpy array generated by np.asarray(image)
        xmin: x coordinate (in geometry coordinate system) of leftmost point
            in georeferenced image
        xlen: maximum - minimum x coordinate in georeferenced image
        ymin: minimum y coordinate in georeferenced image
        ylen: maximum - minimum y coordinate in georeferenced image
        img_xmin: minimum x coordinate in img_arr (should probably be 0)
        img_xlen: maximum - minimum x coordinate in img_arr
        img_ymin: minimum y coordinate in img_arr
        img_ylen: maximum - minimum y coordinate in img_arr
        
    Output: pixel value (array)
        '''
    # coordinate transform calculation, where floor is used to prevent indices 
    # from going out of bounds (also this is proper practice for the accuracy 
    # of the transform)
    x = math.floor((pt.x - xmin) * img_xlen / xlen + img_xmin)
    y = math.floor((ymin-pt.y) * img_ylen / ylen + img_ylen - img_ymin)

    return img_arr[y][x]

def isBlack(color):
    ''' Returns True iff all 3 RGB values are less than 25.
    
    Arguments: 
        color: a list whose first 3 elements are RGB.'''
        
    return (color[0] < 25 and color[1] < 25 and color[2] < 25)

def random_pt_in_triangle(triangle):
    ''' This function outputs a uniformly random point inside a triangle
    (given as a Shapely polygon), according to the algorithm 
    described at http://mathworld.wolfram.com/TrianglePointPicking.html''
    
    Argument:
        triangle: Shapely polygon
        
    Output: Shapely Point drawn randomly from inside triangle'''
    
    # get list of vertices (cut off last element; first point is repeated)
    vertices = np.asarray(triangle.boundary.coords)[:3]
    
    # assuming that vertices[0] is at (0,0), get coordinates of other vertices
    v_1 = vertices[1] - vertices[0]
    v_2 = vertices[2] - vertices[0]
    
    # select random point in parallelogram created by vectors v_1 and v_2
    # r,s are random in [0, 1)
    r = np.random.random_sample()
    s = np.random.random_sample()
    pt = Point(vertices[0] + r * v_1 + s * v_2)
    
    # refelct pt to put it in the triangle if it is not inside
    if not triangle.contains(pt):
        pt = Point(vertices[0] + (1-r) * v_1 + (1-s) * v_2)
        
    # return the random point
    return pt


def most_common_color(poly, img_arr, xmin, xlen, ymin, ylen, sample_limit):
    ''' This function uses pixel sampling to calculate (with high probability)
    the most common RGB value in the section of an image corresponding to 
    a Shapely polygon within the reference geometry.
    
    Arguments:
        poly: Shapely polygon within reference geometry
        img_arr: numpy array generated by np.asarray(image)
        xmin: x coordinate (in geometry coordinate system) of leftmost point
            in georeferenced image
        xlen: maximum - minimum x coordinate in georeferenced image
        ymin: minimum y coordinate in georeferenced image
        ylen: maximum - minimum y coordinate in georeferenced image
        sample_limit: maximum number of pixels to sample before guessing
        
    Output:
        integer corresponding to 256^2 R + 256 G + B
    '''
    
    # triangulate polygon
    triangles = shp.ops.triangulate(poly)
    
    # in very rare cases, the polygon is so small that this shapely operation
    # fails to triangulate it, so we return 0 (black)
    if len(triangles) == 0:
        return 0
    
    # make list of partial sums of areas so we can pick a random triangle
    # weighted by area
    areas = np.asarray([0])
    for triangle in triangles:
        areas = np.append(areas, areas[-1] + triangle.area)
    
    # scale so last sum is 1
    areas = areas / areas[-1]

    # initialize data to monitor throughout the sampling process
    # colors is a dictionary to store the number of pixels of each color
    colors = {}
    sampled = 0
    used = 0
    color_to_return = None
    stop_sampling = False
    # sample as long as none of the stop criteria have been reached
    while not stop_sampling:
        
        # update sample count
        sampled += 1
        
        # select a random triangle (weighted by area) in the triangulation
        r = np.random.random_sample()
        triangle = triangles[np.searchsorted(areas,r)-1]
        
        # select a point uniformly at random from this triangle
        pt = random_pt_in_triangle(triangle)
        
        # gets size of img_arr to align it with poly
        img_xlen = len(img_arr[0])
        img_ylen = len(img_arr)
        
        # get color of pixel that corresponds to pt
        color = pt_to_pixel_color(pt, img_arr, xmin, xlen, ymin, ylen, \
                0, img_xlen, 0, img_ylen)
        

        # in case all are black, return black
        colors[0] = 1
        
        # if not black, add color to dictionary
        if not isBlack(color):    
            
            used += 1
            
            # for hashing
            color_int = 256*256*color[0] + 256*color[1]+color[2]
            
            # update dictionary
            if color_int not in colors:
                colors[color_int] = 0
            colors[color_int] += 1
        
        # decide if we are done sampling (every 10 samples)
        if (sampled % 10 == 0):
            
            # find the most common color and its frequency
            common = max(colors.items(), key=operator.itemgetter(1))[0]
            common_count = colors[common]
            
            # calculate z-score based on proportion test
            # trying to get evidence that this color is over 50% frequent
            # among all pixels
            z_score = (2 * common_count / used - 1) * np.sqrt(used)
            
            # stop sampling if we have convincing evidence or we hit our limit
            if (z_score > 4 or sampled >= sample_limit):
                color_to_return = common
                stop_sampling = True
    
    return color_to_return

def split_noncontiguous(df, cols_to_copy=[]):
    ''' Splits noncontiguous geometries in a dataframe and adds all polygons
    as their own attribute.
    
    Arguments:
        df: geodataframe
        cols_to_copy: columns from non-contiguous region to copy over to 
            all parts that were split off
    
    Output: df with no noncontiguous geometries
    '''
    
     # Initialize indexes to drop
    drop_ix = []
    
    # Iterate through every precinct
    for i, _ in df.iterrows():
        # Check if it precinct is a MultiPolygon
        if df.at[i, 'geometry'].type == 'MultiPolygon':
            # Add index as the index of a row to be dropped
            drop_ix.append(i)
            
            # get shape and area of current precinct
            precinct = df.at[i, 'geometry']
    
            # Iterate through every contiguous region in the precinct
            for region in precinct.geoms:
                # Set geometry of new shape, copy necessary fields
                d ={}
                d['geometry'] = region
                for col in cols_to_copy:
                    d[col] = df.at[i, col]
                df = df.append(d, ignore_index=True)
                
    # Remove original noncontiguous precincts
    df = df.drop(drop_ix)
    
    return df

def fraction_shared_perim(nbrs, indices, perim):
    ''' Helper function for merge_geometries to calculate the fraction
    of a shape's perimeter that is shared with shapes at certain indices.
    Relies on having a dictionary of neighbors and shared perimeters.
    
    Arguments: 
        nbrs: dictionary with keys as neighbor indices, values as shared perims
        indices: indices for which we care about the fraction shared
        perim: perimeter of shape
    '''
    # calcluate total perimeter shared with shapes at indices
    shared_perim = sum([nbrs[key] for key in nbrs if key in indices])
    
    return shared_perim/perim

def merge_geometries(df, indices_to_merge, cols_to_add=[]):
    '''
    Merges the geometries for given indices in a geodataframe into another
    geometry in the dataframe. Merges geometries by longest shared perimeter.
    
    Arguments:
        df: geodataframe
        indices_to_merge: indices of geometries that we want to merge into 
            some other geometry
        cols_to_add: columns in df to add as we merge (e.g. population)
    '''
    # Get neighbors dicts with shared_perims
    df = get_shared_perims(df)
    
    # Delete duplicates from indices_to_merge, which should never exist in
    # the first place but it would ruin the next part so let's be safe
    indices_to_merge = list(set(indices_to_merge))
    
    # temp list because we will be deleting
    original_indices_to_merge = indices_to_merge[:]
    
    # Merge while there are still geometries to merge
    while len(indices_to_merge) > 0:
        
        # find the index of the geometry to merge next, which is the geometry
        # with smallest fraction of its perimeter assigned to indices_to_merge
        fractions = [fraction_shared_perim(df.at[id, 'neighbors'],\
                                           indices_to_merge,\
                                           df.at[id, 'geometry'].length)\
                    for id in indices_to_merge]
        i = indices_to_merge[fractions.index(min(fractions))]

        # update neighbors and shared_perims
        cur_prec = df.at[i, 'neighbors']
        ix = max(cur_prec, key=cur_prec.get)
        merge_prec = df.at[ix, 'neighbors']

        # merge dictionaries
        merge_prec = Counter(merge_prec) + Counter(cur_prec)

        # remove key to itself
        merge_prec.pop(ix)

        # set neighbor dictionary in dataframe
        df.at[ix, 'neighbors'] = merge_prec
        
        # merge geometry
        df.at[ix, 'geometry'] = df.at[ix, 'geometry'].union\
            (df.at[i, 'geometry'])
        
        # delete neighbor reference to i and add reference for merge to key
        for key in list(cur_prec):
            df.at[key, 'neighbors'].pop(i)
            
            ##-----------------------------------------------------------------
            # get perimeter length for key in merge and set in 
            # neighbor list
            key_dist = df.at[ix, 'neighbors'][key]
            df.at[key, 'neighbors'][ix] = key_dist
            
        # remove i from indices to merge
        indices_to_merge.remove(i)
        
    # delete all merged precincts
    df = df.drop(original_indices_to_merge)
    
    return df
    
def merge_to_right_number(df, num_regions):
    ''' Decreases the number of attributes in a dataframe to a fixed number by
    merging the smallest geometries into the neighbor with which it shares the
    longest border.  Also creates 'region' field.
    '''
    # reset index for df
    df = df.reset_index(drop=True)
    
    # Get neighbors dicts with shared_perims
    df = get_shared_perims(df)

    # get list of precinct indices to merge (smallest areas)
    for i, _ in df.iterrows():
        df.at[i, 'area'] = df.at[i, 'geometry'].area
    arr = np.array(df['area'])
    
    precincts_to_merge = arr.argsort()[ : -num_regions]
    
    # merge precincts_to_merge
    df = merge_geometries(df, precincts_to_merge)
    
    # reset index for df
    df = df.reset_index(drop=True)
        
    # set region values
    for i in range(len(df)):
        df.at[i, 'region'] = i
        
    return df

def save_shapefile(df, file_path, cols_to_exclude=[]):
    ''' Saves a geodataframe to shapefile, deletes columns specified by user.
    If the path already exists a backup will be created in the path ./Backup/
    
    Arguments:
        df: geodataframe to be written to file
        file_str: string file path
        cols_to_exclude: columns from df to be excluded from attribute table
            (possibly because it cannot be written, like an array)
            
    Output: 1
    '''
    # make temporary dataframe so we can exclude columns
    df = df.drop(columns=cols_to_exclude)
    
    # Attempt to fix object types in dataframe by converting to float if
    # possible
    for col in df.columns:
        try:
            df[col] = df[col].astype(float)
        except:
            continue

    # Create backup if path already exists
    if os.path.exists(file_path):
        backup_dir = '/'.join(file_path.split('/')[:-1]) + '/Backup'

        # Create backup folder if it does not already exist
        if not os.path.exists(backup_dir):
            os.mkdir(backup_dir)

        # Get current date
        t = datetime.datetime.now()
        d = str(t.month) + '-' + str(t.day) + '-' + str(t.year) + '_' + \
            str(t.hour) + '-' + str(t.minute)        
        
        # Save old file to backup folder
        filename = file_path.split('/')[-1]
        file_no_ext = '.'.join(filename.split('.')[:-1])
        file_ext = filename.split('.')[-1]
        backup_path = backup_dir + '/' + file_no_ext + '_' + d + '.' + file_ext
        shutil.copy(file_path, backup_path)
        
        # Save the new file to the folder
        df.to_file(file_path)

    # Save file if the file does not already exist
    else:
        df.to_file(file_path)
    
    return 1
    
def delete_cpg(path):
    '''Deletes the CPG with a corresponding SHP. ArcGIS sometimes incorrectly
    encodes a shapefile and incorrectly saves the CPG. Before running most
    of the scripts, it is beneficially to ensure an encoding error does throw
    an error
    
    Argument
        path: path to a file that has the same name as the .cpg file. Usually
        the shapefile
    '''
    
    cpg_path = '.'.join(path.split('.')[:-1]) + '.cpg'
    if os.path.exists(cpg_path):
        os.remove(cpg_path)
    
def default_path(path, local, direc_path):
    '''If the path is a keyword, the path to a designated shapefile will be
    returned. The possible default keywords are census_block, precinct,
    precinct_final
    
    Arguments:
        path: current path to the shapefile. Might be a default keyword
        local: name of the locality, which will be the parent directory
        direc_path: directory path to all of the locality folders
    '''
    
    if path == 'census_block':
        filename = local + '_census_block.shp'
        filename = filename.replace(' ', '_')
        path = direc_path + '/' + local + '/' + filename
                        
    if path == 'precinct':
        filename = local + '_precincts.shp'
        filename = filename.replace(' ', '_')
        path = direc_path + '/' + local + '/' + filename
        
    if path == 'precinct_final':
        filename = local + '_precincts_final.shp'
        filename = filename.replace(' ', '_')
        path = direc_path + '/' + local + '/' + filename

    return path
    
def set_CRS(gdf, new_crs='epsg:4269'):
    ''' This function will set a coordinate reference system (CRS) for a a 
    geodataframe
    
    Arguments
        gdf: This is the geodataframe that we are converting to a different
                coordinate reference systems
        new_crs: This is the CRS we are converting to. This is usually in the
        form epsg:####
    '''
    
    # If no CRS set, set it with .crs
    if gdf.crs == None:
        gdf.crs = {'init': new_crs}
    
    # Transform CRS
    else:
        gdf = gdf.to_crs({'init': new_crs})
        
    return gdf

def read_one_csv_elem(csv_path, row=0, col=1):
    ''' This function will return one element of the csv
    
    Arguments:
        csv_path: path to read in the csv_file
        row: row of the text to read in
        col: col of the text to read in
    '''
    with open(csv_path) as f:
        reader = csv.reader(f)
        data = [r for r in reader]
    return data[row][col]

def read_csv_to_df(csv_path, head, col_names, list_cols):
    ''' Read in a csv for batching for other processes
    
    Arguments
        csv_path: path to read in the csv file
        head: the header row to read in the csv as a pandas df
        col_names: list column names in order as headers for the pandas df
        list_cols: columns to convert to lists from comma delimited strings
    '''
    # Read in csv as df
    csv_df = pd.read_csv(csv_path, header=head, names=col_names)
    
    # Convert comma delimited string columns to lists
    for col in list_cols:
        if col in col_names:
            csv_df[col] = csv_df[col].str.split(',')
            
    return csv_df

def majority_areal_interpolation(df_to, df_from, adjust_cols):
    ''' Perform majority area areal interpolation on two dataframes. Returns
    the modified dataframe (to_df). Also assigns element that do not have
    overlapping bounding boxes by closest centroid distance
    
    Arguments:
        to_df_path: path to the shapefile containing the dataframe to be 
        modified
        from_df_path: path to the shapefile used to modify to_df
        adjust_cols: list of tuples that determine which df_to columns are
        set equal to which df_from cols. Format column is string manipulation
        to apply such as upper/lower/title case [(df_to_col1, df_from_col1, 
        format_col1), (df_to_col2, df_from_col2, format_col2),...]
            
    Output: To dataframe with the value interpolated'''

    # Read in input dataframe
    df_from.index = df_from.index.astype(int)
    
    # Need to define which columns in the to dataframe to drop. We will drop
    # all columns from the csv that actually exist in the to dataframe. We
    # will also drop columns in the to_
    drop_cols_before = []
    drop_cols_after = []
    for tup in adjust_cols:
        # add to before drop
        if tup[0] in df_to.columns:
            drop_cols_before.append(tup[0])
            
        # add to after drop
        if tup[1] not in df_from.columns:
            print('Column not in from df: ' + tup[1])
            drop_cols_after.append(tup[0])
            
    # Drop columns that are already in df_to
    df_to = df_to.drop(columns=drop_cols_before)

    # Create all output columns in the to_df
    for tup in adjust_cols:
        df_to[tup[0]] = pd.Series(dtype=object)

    # construct r-tree spatial index. Creates minimum bounding rectangle about
    # each geometry in df_from
    si = df_from.sindex

    # get centroid for al elements in df_from to take care of no intersection
    # cases
    df_from['centroid'] = pd.Series(dtype=object) 
    for j, _ in df_from.iterrows():
        df_from.at[j, 'centroid'] = df_from.at[j, 'geometry'].centroid

    # iterate through every geometry in the to_df to match with from_df and set
    # target values
    for i, _ in df_to.iterrows():
    
        # initialize current element's geometry and check which for minimum
        # bounding rectangle intersections
        df_to_elem_geom = df_to.at[i, 'geometry']
        poss_df_from_elem = [df_from.index[i] for i in \
                      list(si.intersection(df_to_elem_geom.bounds))]

        # If precinct's MBR only from_df geometry. Set it equal
        if len(poss_df_from_elem) == 1:
            df_from_elem = poss_df_from_elem[0]
        else:
            # for cases with multiple matches, compare fractional area
            frac_area = {}
            found_majority = False
            for j in poss_df_from_elem:
                if not found_majority:
                    area = df_from.at[j, 'geometry'].intersection(\
                                   df_to_elem_geom).area / \
                                   df_to_elem_geom.area
                    # Majority area means, we can assign
                    if area > .5:
                        found_majority = True
                    frac_area[j] = area

            # if there was intersection get max of frac area
            if len(frac_area) > 0:
                df_from_elem = max(frac_area.items(), \
                                 key=operator.itemgetter(1))[0]
            # No intersection so found nearest centroid
            else:
                # get centroid on the current geometry
                c = df_to.at[i, 'geometry'].centroid
                min_dist = -1
                
                # find the minimum distance index
                for j, _ in df_from.iterrows():
                    cur_dist = c.distance(df_from.at[j, 'centroid'])
                    if min_dist == -1 or cur_dist < min_dist:
                        df_from_elem = j
                        min_dist = cur_dist

        # Set corresponding df_to values to df_from values if the column exist
        # in from_df     
        df_from_cols = df_from.columns
        for tup in adjust_cols:
            # Interpolate
            if tup[1] in df_from_cols:
                input_str = df_from.at[df_from_elem, tup[1]]

                # Set formatting from input
                if tup[2] == 'U':
                    input_str = input_str.upper()
                elif tup[2] == 'L':
                    input_str = input_str.lower()
                elif tup[2] == 'T':
                    input_str = titlecase(input_str)

                df_to.at[i, tup[0]] = input_str

    # Delete and print columns that are missing in from dataframe
    df_to = df_to.drop(columns=drop_cols_after)

    # Return output dataframe
    return df_to

def shp_from_sampling(local, num_regions, shape_path, out_path, img_path, \
                      colors=0, sample_limit=500):
    ''' Generates a precinct level shapefile from census block data and an 
    image cropped to a locality's extents. Also updates the attribute table in
    the census block shapefile to have a region value that represents to 
    precinct id.
    
    Arguments:
        local: name of the locality
        num_regions: number of precincts in the locality
        shape_path: full path to the census block shapefile
        out_folder: directory that precinct level shapefile will be saved in
        img_path: full path to image used to assign census blocks to precincts
        
    Output:
        Number of census blocks in the county
        '''        
    # Convert image to array, color reducing if specified
    img = Image.open(img_path)
    if colors > 0:
        img = reduce_colors(img, colors)
    img_arr = np.asarray(img)

    # Delete CPG file if it exists
    delete_cpg(shape_path)
    
    # read in census block shapefile
    df = gpd.read_file(shape_path)

    # Create new series in dataframe
    add_cols = ['color', 'region']
    for i in add_cols:
        df[i] = pd.Series(dtype=object)
    
    # Calculate boundaries of the geodataframe using union of geometries
    bounds = shp.ops.cascaded_union(list(df['geometry'])).bounds
    
    # Calculate global bounds for shape
    shp_xlen = bounds[2] - bounds[0]
    shp_ylen = bounds[3] - bounds[1]
    shp_xmin = bounds[0]
    shp_ymin = bounds[1]
    
    # Iterate through each polygon and assign its most common color
    for i, _ in df.iterrows():
        
        # Get current polygon
        poly = df.at[i, 'geometry']
        
        # Set color for census block
        df.at[i, 'color'] = most_common_color(poly, img_arr, shp_xmin, \
             shp_xlen, shp_ymin, shp_ylen, sample_limit)
            
    # Assign each polygon with a certain color a district index
    for i, color in enumerate(df['color'].unique()):
        df.loc[df['color'] == color, 'region'] = i
    
    # Get unique values in the df ID column
    prec_id = list(df.region.unique())
    
    # Initialize the precinct dataframe, which will eventually be exported
    # as the precinct shapefile
    df_prec = pd.DataFrame(columns=['region', 'geometry'])
    
    # Iterate through all of the precinct IDs and set geometry of df_prec with
    # cascaded union
    for i in range(len(prec_id)):
        df_poly = df[df['region'] == prec_id[i]]
        polys = list(df_poly['geometry'])
        df_prec.at[i, 'geometry'] = shp.ops.cascaded_union(polys)
        df_prec.at[i, 'region'] = prec_id[i]
        
    # Split noncontiguous precincts
    df_prec = split_noncontiguous(df_prec)

    # Merge precincts fully contained in other precincts
    df_prec = merge_fully_contained(df_prec)

    # Merge precincts until we have the correct number of precincts
    df_prec = merge_to_right_number(df_prec, num_regions)
    
    # Convert precinct dataframe into a geodataframe
    df_prec = gpd.GeoDataFrame(df_prec, geometry='geometry')

    # Assign census blocks to regions
    df = majority_areal_interpolation(df, df_prec, [('region', 'region', 0)])
    
    # Save census block shapefile with updated attribute table
    save_shapefile(df, shape_path, cols_to_exclude=['color'])
    
    # Save precinct shapefile    
    save_shapefile(df_prec, out_path, ['neighbors'])
        
    return len(df)
